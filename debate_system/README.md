# ğŸ§  Autonomous Multi-LLM Markdown Debate Engine

This project simulates a full-scale multi-agent debate using local LLMs via **Ollama**, with agents reasoning entirely in **Markdown** and a fully traceable architecture. Supports multiple turn strategies including round-robin and Delphi consensus, with tiered memory management, contradiction detection, argument trees, and more.

---

## ğŸš€ Features

- Persona agents with evolving beliefs and roles
- Fully autonomous loop: turn selection â†’ LLM reasoning â†’ belief update
- Multi-tiered memory system (STM, LTM, belief memory)
- Supports `round_robin`, `priority`, `interrupt`, `delphi` turn strategies
- Advanced contradiction detection with vector similarity and LLM verification
- Memory summarization for handling long-running debates
- Recovery, replay, and persistent session storage
- Argument tree generation and export (Markdown + JSON)
- Real-time Streamlit UI with streaming responses
- Designed for local models (uses `gemma3:latest` via Ollama)

---

## ğŸ§± Architecture

```plaintext
debate_system/
â”œâ”€â”€ app/                  # Core logic
â”‚   â”œâ”€â”€ persona_agent.py      # Agent implementation
â”‚   â”œâ”€â”€ mediator_agent.py     # Mediator/synthesis agent
â”‚   â”œâ”€â”€ memory_manager.py     # Centralized memory manager
â”‚   â”œâ”€â”€ agent_state_tracker.py# Agent state/memory tracking
â”‚   â”œâ”€â”€ contradiction_detector.py # Detects belief contradictions
â”‚   â”œâ”€â”€ context_builder.py    # Manages context window allocation
â”‚   â”œâ”€â”€ consensus_engine.py   # Consensus generation
â”‚   â”œâ”€â”€ delphi_engine.py      # Delphi method implementation
â”‚   â”œâ”€â”€ argument_graph.py     # Tree of debate points
â”‚   â”œâ”€â”€ debate_manager.py     # Orchestrates the debate
â”‚   â”œâ”€â”€ flow_control.py       # Turn management strategies
â”‚   â”œâ”€â”€ core_llm.py           # LLM client wrapper
â”‚   â”œâ”€â”€ performance_logger.py # Performance tracking
â”‚   â”œâ”€â”€ logger.py             # Logging functionality
â”‚   â””â”€â”€ main.py               # CLI entry point
â”œâ”€â”€ memory/                # Memory systems
â”‚   â”œâ”€â”€ mongo_store.py         # MongoDB-based STM and beliefs
â”‚   â”œâ”€â”€ qdrant_store.py        # Vector DB for LTM and RAG
â”‚   â””â”€â”€ embeddings.py          # Embedding utilities
â”œâ”€â”€ ui/                   # Streamlit frontend
â”‚   â”œâ”€â”€ streamlit_app.py      # Main UI
â”‚   â””â”€â”€ pages/                # UI components
â”œâ”€â”€ plugins/              # Custom validators/tools
â”œâ”€â”€ sessions/             # Output folder per run
â”œâ”€â”€ tests/                # Pytest-based validation
â”œâ”€â”€ docker-compose.yaml   # MongoDB & Qdrant containers
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## âš™ï¸ Setup

### Prerequisites

1. **Ollama**: Install [Ollama](https://ollama.com) and pull the required model:

```bash
ollama pull gemma3:latest
```

2. **MongoDB & Qdrant**: You can run these using Docker:

```bash
cd debate_system
docker-compose up -d
```

Or install them locally following their documentation.

### Python Environment

Create and activate your Python environment:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## âœ… Usage

### 1. Launch the UI

Run the following command from the `debate_system` directory:

```bash
python -m streamlit run ui/streamlit_app.py
```

Upload a `.yaml` config file and press â€œStart Debateâ€.

### 2. CLI / Dev Usage

```bash
python3 app/main.py
```

### 3. Explore Output

Results will be saved in:

```plaintext
sessions/{session_id}/
â”œâ”€â”€ summary.md            # Markdown debate log
â”œâ”€â”€ summary.json          # Structured output
â”œâ”€â”€ turns.log             # Individual turn logs
â”œâ”€â”€ performance_log.json  # Performance metrics
â””â”€â”€ argument_graph.json   # Structured argument tree
```

---

## ğŸ§  Memory System

The debate system uses a multi-tiered memory architecture:

1. **Short-Term Memory (STM)** - Recent conversation turns stored in MongoDB
2. **Long-Term Memory (LTM)** - Historical semantic information stored in Qdrant
3. **Belief Memory** - Agent's core beliefs and contradictions
4. **RAG Retrieval** - Vector-based knowledge retrieval

Memory is dynamically summarized to fit within context windows, with intelligent token allocation between different memory components.

---

## ğŸ§ª Testing

```bash
pytest tests/
```

---

## ğŸŒŸ Default LLM Setup

All agents run `gemma3:latest` via Ollama, with real-time streamed responses. The system is designed to work with:

- Context window: 4096 tokens (configurable)
- Response reserve: 1024 tokens
- Customizable token allocation ratios

---

## âœ¨ Contributions

PRs and feature suggestions welcome. Future roadmap includes:

- Live argument graph visualization
- Custom tool integration
- Agent finetuning via debate feedback
- Support for additional local models

---

## ğŸ§  Maintained by

Gil B. 
